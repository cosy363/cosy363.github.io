<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on ✍(◔◡◔)</title>
        <link>/posts/</link>
        <description>Recent content in Posts on ✍(◔◡◔)</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko</language>
        <lastBuildDate>Mon, 08 Jul 2024 12:00:00 +0000</lastBuildDate>
        <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>SonarQube Jenkins 설치 및 연동</title>
            <link>/posts/sonarqube/</link>
            <pubDate>Mon, 08 Jul 2024 12:00:00 +0000</pubDate>
            
            <guid>/posts/sonarqube/</guid>
            <description>Key Feature
Github 레포지토리의 main 브랜치로 푸시가 발생하면, GitHub Webhook이 Jenkins를 호출. Jenkins 서버에서 소스코드를 가져와 SonarQube Scanner로 코드에 대한 정적 분석 수행. SonarQube 서버로 정적 분석 결과 전송 Jenkins, Sonarqube 연동 1. Github 액세스 토큰 생성 -&amp;gt; Github 레포별 액세스 토큰 생성
1. Settings &amp;gt; Developer Settings &amp;gt; Personal Access Tokens &amp;gt; (Fine Grained) - Generate New Token &amp;gt; Token (Classic) - Select Scopes - `repo`, `admin:repo_hook` 2. 토큰 값 별도 저장 2.</description>
            <content type="html"><![CDATA[<p>Key Feature</p>
<ul>
<li>Github 레포지토리의 <code>main</code> 브랜치로 푸시가 발생하면, GitHub Webhook이 Jenkins를 호출.</li>
<li>Jenkins 서버에서 소스코드를 가져와 SonarQube Scanner로 코드에 대한 정적 분석 수행.</li>
<li>SonarQube 서버로 정적 분석 결과 전송</li>
</ul>
<h1 id="jenkins-sonarqube-연동">Jenkins, Sonarqube 연동</h1>
<h2 id="1-github-액세스-토큰-생성">1. Github 액세스 토큰 생성</h2>
<p><img src="https://github.com/opmate/opm-insider/assets/80619766/eb2f9f20-03d4-487f-9558-77a8418977b9" alt="image"></p>
<p>-&gt; Github 레포별 액세스 토큰 생성</p>
<pre><code>1. Settings &gt; Developer Settings &gt; Personal Access Tokens &gt; (Fine Grained)
	- Generate New Token &gt; Token (Classic)
	- Select Scopes
		- `repo`, `admin:repo_hook`
2. 토큰 값 별도 저장
</code></pre>
<h2 id="2-github-webhook-설정">2. Github Webhook 설정</h2>
<p>-&gt; 각 레포지토지에서 Webhook 설정</p>
<pre><code>1. Repository &gt; Settings &gt; Webhook &gt; Add Webhook
	- Payload URL : http://{public ip}:8080/github-webhook/
	- Content Type: `application/json`
</code></pre>
<h2 id="3-jenkins에서-github-credentials-생성">3. Jenkins에서 Github Credentials 생성</h2>
<p>-&gt; Github 접근용 Credential 생성</p>
<pre><code>1. Jenkins &gt; Jenkins 관리 &gt; Credentials
2. Domain &gt; (global) 클릭 후 Add Credentials
	- Username: {Github 계정}
	- Password: {1번 단계에서 생성했던 토큰값 입력}
	- ID: {Credential 이름}
</code></pre>
<h2 id="4-sonarqube-프로젝트-생성">4. Sonarqube 프로젝트 생성</h2>
<p>-&gt; 각 레포지토리 별로 1개씩 생성</p>
<pre><code>1. Sonarqube &gt; Create Project &gt; Local Project 
	- Use Global Setting
</code></pre>
<h2 id="5-sonarqube-서버-토큰-생성">5. Sonarqube 서버 토큰 생성</h2>
<p>-&gt; Sonarqube 서버 접근용 토큰을 생성</p>
<pre><code>1. Sonarqube &gt; My Account(우상단 버튼) &gt; Security
2. Generate Tokens &gt; Name, Type, Expires in 입력 후 Generate 
	- Global Analysis Token: 모든 프로젝트에 대한 권한
	- User Token: 토큰을 생성한 유저가 가지고 있는 모든 권한
	- Project Token: 특정 프로젝트에 대한 권한 (권장)
3. Token 값 저장
</code></pre>
<h2 id="6-jenkins-sonarqube-server-관련-설정">6. Jenkins: Sonarqube Server 관련 설정</h2>
<p>-&gt; SonarQube 서버 및 SonarQube Scanner 설정</p>
<pre><code>- Jenkins Dashboard &gt; Jenkins 관리 &gt; System
	1. Sonarqube servers
		- Environment variables: ✔️
		- SonarQube Installiation &gt; Add SonarQube
			- Name: {Sonarqube Server Name} // This is applied to
			- Server URL: {Sonarqube Server URL}
			- Server Authentication Token: Add &gt; Jenkins
				- Secret Text &gt; 5번 단계에서 생성한 토큰값 입력

- Dashboard &gt; Jenkins 관리 &gt; Tools
	- Sonarqube Scanner Installations (안보이면 Plugin Install)
		- Add SonarQube Scanner &gt; Install Automatically
</code></pre>
<h2 id="7-jenkins-pipeline-생성">7. Jenkins Pipeline 생성</h2>
<pre><code>- Jenkins Dashboard &gt; 새로운 Item &gt; Pipeline
	- Github Project: ✔️
		- Project URL: {Repository URL}
	- Build Trigger &gt; Github hook trigger for GITScm Polling
	- Pipeline &gt; Pipeline script 혹은 Pipeline script from SCM
</code></pre>
<p>Pipeline Script 내용</p>
<pre tabindex="0"><code>pipeline {
    agent any
    stages {
        stage(&#39;Checkout&#39;) {
            steps {
                // credentialsID는 3번 단계에서 생성한 Github Credentials, url은 대상 Repository URL 
                git branch: &#39;main&#39;, credentialsId: &#39;OPMATE&#39;, url: &#39;https://github.com/opmate/opm-v2-cli.git&#39; 
            }
        }

		stage(&#39;SonarQube analysis&#39;) {
			steps {
				script {
					scannerHome = tool &#39;sonar_scanner&#39; // 6번 단계에서 생성한 SonarQube Scanner Name 명시
				}
				withSonarQubeEnv(&#39;Sonarqube_OPMATE&#39;) { // 6번 단계에서 설정한 SonarQube Installation Name 명시

                // -Dsonar.projectKey는 4번 단계에서 생성한 Sonarqube 프로젝트 Key, -Dsonar.sources는 분석할 소스 디렉토리
				sh &#34;${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=OPMATE_CLI -Dsonar.sources=. &#34; 
				
				}
			}
        }

    }
}
</code></pre><p>opmateSCA1!</p>
<h1 id="result">Result</h1>
<p><img src="https://github.com/opmate/opm-insider/assets/80619766/4f15d044-2005-4ef2-8161-06105a901919" alt="Jenkins + Sonarqube 정적분석 파이프라인 구현-1"></p>
<p><img src="https://github.com/opmate/opm-insider/assets/80619766/8e1bcbcb-9f01-4929-b5e7-c96c23aedea1" alt="image"></p>
<p>![[무제 파일.png]]![[Pasted image 20240610144309.png]]</p>
]]></content>
        </item>
        
        <item>
            <title></title>
            <link>/posts/1/</link>
            <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
            
            <guid>/posts/1/</guid>
            <description>HOW IS VPC IMPLEMENTED?
https://brunch.co.kr/@growthminder/104
태초엔 EC2만이 있었다. 대클라우드 전환의 시대에서, 온프레미스 데이터센터에서 넘어오는 고객은 2가지 상황을 보통 직면하게 된다.
클라우드는 리소스를 공유함으로써 리소스를 절약한다 -&amp;gt; 클라우드에서의 서버는 &amp;lsquo;VM&amp;rsquo;이다. &amp;lsquo;VM&amp;rsquo;은 하나의 물리 서버를 공유한다. -&amp;gt; 초기에 이 &amp;lsquo;VM&amp;rsquo;은 &amp;lsquo;EC2&amp;rsquo;라는 형태로 제공되었다. -&amp;gt; 하지만 이 EC2 인스턴스를 추가, 수정할 때마다 라우팅 테이블을 계속 업데이트해야된다. -&amp;gt; 인스턴스 플릿의 크기가 커질수록 라우팅 테이블의 크기가 증가한다. 또한 업데이트를 할 때 트래픽이 필연적으로 발생한다. =&amp;gt; Bottleneck!</description>
            <content type="html"><![CDATA[<p>HOW IS VPC IMPLEMENTED?</p>
<p><a href="https://brunch.co.kr/@growthminder/104">https://brunch.co.kr/@growthminder/104</a></p>
<ol>
<li>태초엔 EC2만이 있었다.</li>
</ol>
<p>대클라우드 전환의 시대에서, 온프레미스 데이터센터에서 넘어오는 고객은 2가지 상황을 보통 직면하게 된다.</p>
<p>클라우드는 리소스를 공유함으로써 리소스를 절약한다
-&gt; 클라우드에서의 서버는 &lsquo;VM&rsquo;이다. &lsquo;VM&rsquo;은 하나의 물리 서버를 공유한다.
-&gt; 초기에 이 &lsquo;VM&rsquo;은 &lsquo;EC2&rsquo;라는 형태로 제공되었다.
-&gt; 하지만 이 EC2 인스턴스를 추가, 수정할 때마다 라우팅 테이블을 계속 업데이트해야된다.
-&gt; 인스턴스 플릿의 크기가 커질수록 라우팅 테이블의 크기가 증가한다. 또한 업데이트를 할 때 트래픽이 필연적으로 발생한다. =&gt; Bottleneck!</p>
<p>-&gt; 또한 만약 온프레미스 데이터 센터와 AWS EC2 Fleet의 사설 네트워크가 겹친다면 라우팅 문제가 발생한다. 그리고 보통 CIDR 대역은 이미 정한 Rule이므로
쉽게 바꾸기도 힘들다.
-&gt; Double NAT로 해결할 수도 있지만, 이런 네트워크 구조를 네트워크 엔지니어들에게 짜달라고 하면 싸다구를 맞게 될 것이다.</p>
<ol start="2">
<li>VPC의 등장</li>
</ol>
<p>이러한 문제를 해결하기 위해 VPC(Virtual Private Cloud)라는 서비스가 등장한다.
고객이 직접 IP Range를 선택할 수 있고, 외부 연결을 위한 Route Aggregation을 지원한다.
또한 가장 큰 장점은 고객이 기존에 가지고 있는 네트워크와 Conformance를 가진다는 것이다.</p>
<ol start="3">
<li>VPC의 구현</li>
</ol>
<p>그렇다면 AWS는 이러한 VPC를 어떻게 구현할까?</p>
<p>단순히 라우터를 둔다고 가정해보자.
아무리 좋은 라우터를 둔다고 해도 문제가 생길 것이다.</p>
<p>성능이 빵빵한 초얼짱라우터의 예시를 들어보자:
초얼짱라우터 Spec:
가격: +200K$, (대략 2<del>3억원)
VRF(Virtual Routing and Forwarding) : 1000</del>2000개
를 가졌다고 했을 때</p>
<p>여기서 VRF는 일종의 VPC라고 생각하면 된다.</p>
<p>이 때 Configuration을 바꾼다고 하면 단순히 설정 파일의 텍스트 데이터를 파싱하고 Validate하는 데만 3-5분이 걸린다.
심지어 어떤 라우터의 경우 Config의 데이터 구조가 Single Linked List로 구현되어 있어 조회를 하는 데도 많이 걸렸다고 한다.</p>
<p>그리고 심지어 이러한 라우터들은 Scale-out이 용이하지 않다.
또한 VRF 당 VLAN, 즉 서브넷이 할당되는 양이 고정되어 있어 일부 VPC의 경우 리소스가 낭비되는 경우도 있을 것이다.</p>
<p>그리고 사용자가 VPC마다 원하는 대로 IP Range를 정할 수 있다면, AWS는 내부적으로 어떻게 각 VPC의 네트워크 대역을 구분할까?
이러한 VPC들이 같은 물리 서버에 존재할 수 있는데도 말이다.</p>
<p>그리고 라우팅을 한다고 하면, 수천만개에 달하는 전 세계의 VPC들 간의 라우팅을 어떻게 할 수 있는 것일까?</p>
<ol>
<li>VPC의 등장: 요구 사항과 기존 네트워크 구조와의 통합
이런 문제를 해결하기 위해 AWS는 VPC(가상 사설 클라우드)를 도입했습니다. VPC는 고객이 선택한 IP 주소를 기반으로 외부 네트워크와 연결할 수 있는 라우트 집계(Route Aggregation)를 지원해, 기존 네트워크 디자인과 일관성을 유지하면서도 클라우드로의 전환을 쉽게 해줬습니다.</li>
</ol>
<p>이 접근 방식은 데이터를 온프레미스에서 클라우드로 옮기려는 기업에 특히 유용했죠. 기존 온프레미스 데이터센터는 자체 네트워크 환경을 기준으로 설계되었기 때문에, 클라우드와의 통합을 위해서는 새로운 네트워크 구조가 필요했습니다.</p>
<p>Subnet과 VLAN, VPC와 VRF의 유사성
Subnet ~= VLAN: 서브넷은 물리적 네트워크에서 VLAN과 비슷한 역할을 합니다.
VPC <del>= VRF: VPC는 클라우드 네트워크에서 VRF(Virtual Routing and Forwarding)와 비슷한 기능을 합니다. 하지만 VRF는 확장성 문제가 있었습니다. 대형 라우터도 1,000에서 2,000개의 VRF만 지원할 수 있었고, VLAN과 VRF의 비율이 고정되어 있다 보니 서브넷이 낭비되거나 부족한 상황이 발생할 수 있었습니다.
예를 들어, 한 VPC에서 네 개의 서브넷을 구성하는 데 3MB 정도의 설정 파일이 필요하고, 이 파일을 파싱하고 검증하는 데 3</del>5분이 걸린다는 상황이었습니다. 게다가 라우터 소프트웨어가 설정을 단일 연결 리스트(single linked list)로 구현해놔서, 설정이 많아질수록 조회 성능이 점점 더 떨어지는 문제도 있었습니다.</p>
<ol start="3">
<li>네트워크 확장의 필요성: 사일로에서의 용량 문제
AWS는 각 고객의 인스턴스를 랜덤하게 다양한 사일로(silo)에 배치했습니다. 하지만 인스턴스를 추가할 때마다 특정 사일로에 용량이 남아 있어도, 배치 규칙 때문에 다른 사일로에 인스턴스를 배치하지 못하는 상황이 발생하곤 했습니다.</li>
</ol>
<p>이 문제를 해결하기 위해 AWS는 네트워크 확장을 위한 몇 가지 요구 사항을 설정했습니다:</p>
<p>수백만 개의 환경을 확장 가능하게 해야 한다.
리전 내의 어느 서버에서도 VPC에 연결된 인스턴스를 호스팅할 수 있어야 한다.
사일로 문제 해결: 매핑 서비스와 분산 조회 서비스
AWS는 이 문제를 해결하기 위해 **매핑 서비스(Mapping Service)**와 **분산 조회 서비스(Distributed Lookup Service)**를 도입했습니다. 매핑 서비스는 VPC와 인스턴스 IP를 물리적 서버에 매핑해 고객에게 투명하게 처리합니다.</p>
<p>매핑 서비스는 각 인스턴스의 VPC ID와 MAC 주소를 조회해 해당 인스턴스가 어느 서버에 있는지 알려줍니다.
이 매핑 서비스는 각 서버마다 복제본을 가지고 있으며, 캐시 미스가 발생하지 않도록 설계되었습니다.
4. L3 라우팅과 온프레미스 연결
온프레미스 서버와의 통합: VPN 및 Direct Connect
온프레미스 서버와 AWS 간의 연결은 엣지 라우터를 통해 이루어집니다. 엣지 라우터는 IP 패킷을 IPSEC 패킷으로 변환해 보안을 유지하고, Direct Connect를 사용할 경우 802.1Q VLAN 태그가 붙은 패킷으로 변환됩니다. 이를 통해 고객은 안전하게 온프레미스와 클라우드 간 데이터를 주고받을 수 있습니다.</p>
<p>아웃바운드 트래픽 처리: Elastic IP
AWS에서의 아웃바운드 트래픽은 Elastic IP를 통해 외부 네트워크와 연결됩니다. 고객은 VPC 내부의 사설 IP 주소를 사용해 클라우드 내에서 인스턴스 간 통신을 할 수 있으며, 외부와의 통신이 필요할 경우 Elastic IP를 할당받아 사용할 수 있습니다.</p>
<p>결론: 네트워크 확장의 새로운 패러다임
AWS의 VPC는 물리적 네트워크 토폴로지와 완전히 분리된 가상 L2 토폴로지를 구현해 확장성과 유연성을 극대화했습니다. 이런 구조는 클라우드 네트워크에서 더 효율적이고 안전한 네트워크 구성을 가능하게 하고, 고객이 원하는 네트워크 확장을 지원할 수 있도록 돕습니다.</p>
<p>AWS는 앞으로도 네트워크 인프라를 지속적으로 발전시켜 고객의 요구를 충족시키고, 온프레미스와의 원활한 통합을 위해 다양한 기술을 도입할 것입니다.</p>
]]></content>
        </item>
        
    </channel>
</rss>
